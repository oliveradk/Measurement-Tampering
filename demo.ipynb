{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurement Tampering Demo\n",
    "\n",
    "This is a simple implementation of probing for evidence of tamper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load and tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration redwoodresearch--diamonds-seed0-13453d0c0f0868b1\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/redwoodresearch___parquet/redwoodresearch--diamonds-seed0-13453d0c0f0868b1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55bd8cecfb5467aac327c2042eb1986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 1000 examples...\n",
      "Tokenizing 200 examples...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import CodeGenForCausalLM\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = load_dataset(\"redwoodresearch/diamonds-seed0\", \"s0\") # load the first seed\n",
    "\n",
    "max_train_samples = 1000 # This is very low! In the paper we use the 25k points for 5 epochs\n",
    "max_val_samples = 200\n",
    "\n",
    "class MeasurementDataset(Dataset):\n",
    "    max_length = 1024\n",
    "    \n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.measurements = torch.tensor(dataset[\"measurements\"]) # (batch, nb_sensors=3)\n",
    "        self.ground_truth = torch.tensor(dataset[\"is_correct\"]) # (batch,)\n",
    "        self.is_trusted = torch.tensor(dataset[\"is_clean\"]) # (batch,)\n",
    "        \n",
    "        print(f\"Tokenizing {len(dataset['text'])} examples...\")\n",
    "        self.tokenized_text = tokenizer(\n",
    "            dataset[\"text\"],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"input_ids\": self.tokenized_text.input_ids[index],\n",
    "            \"attention_mask\": self.tokenized_text.attention_mask[index],\n",
    "            \"ground_truth\": self.ground_truth[index],\n",
    "            \"measurements\": self.measurements[index],\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ground_truth)\n",
    "\n",
    "model_name = \"Salesforce/codegen-350M-mono\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\" # pad on the left side so that the end of the sequence is always at the same position (for simplicity)\n",
    "\n",
    "train_ds = MeasurementDataset(dataset[\"train\"][:max_train_samples], tokenizer)\n",
    "val_ds = MeasurementDataset(dataset[\"validation\"][:max_val_samples], tokenizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a measurement predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithProbe(torch.nn.Module):\n",
    "    def __init__(self, model: CodeGenForCausalLM, probe: torch.nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.probe = probe\n",
    "    \n",
    "    def embed(self, *args, **kwargs):\n",
    "        return self.model.transformer(*args, **kwargs).last_hidden_state[:, -1, :]\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        last_position_activations = self.embed(*args, **kwargs)\n",
    "        return self.probe(last_position_activations)\n",
    "    \n",
    "model = CodeGenForCausalLM.from_pretrained(model_name).to(device)\n",
    "measurement_predictor = ModelWithProbe(model, torch.nn.Linear(model.config.hidden_size, 3).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [02:21<00:00,  2.24s/it, loss=1.06] \n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(measurement_predictor.parameters(), lr=2e-5)\n",
    "data_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "pbar = tqdm(data_loader)\n",
    "for batch in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    tokens = {\"input_ids\": batch[\"input_ids\"].to(device), \"attention_mask\": batch[\"attention_mask\"].to(device)}\n",
    "    labels = batch[\"measurements\"].float().to(device)\n",
    "    \n",
    "    preds = measurement_predictor(**tokens)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(preds, labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    pbar.set_postfix({\"loss\": loss.item()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the last layer's activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:48<00:00,  1.52s/it]\n",
      "100%|██████████| 7/7 [00:10<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeds = {}\n",
    "with torch.no_grad():\n",
    "    for split, ds in [(\"train\", train_ds), (\"val\", val_ds)]:\n",
    "        embeds[split] = []\n",
    "        dataloader = DataLoader(ds, batch_size=32, num_workers=4)\n",
    "        for batch in tqdm(dataloader):\n",
    "            tokens = {\"input_ids\": batch[\"input_ids\"].to(device), \"attention_mask\": batch[\"attention_mask\"].to(device)}\n",
    "            embeds[split].append(measurement_predictor.embed(**tokens).detach().cpu().numpy())\n",
    "\n",
    "train_embeds = np.concatenate(embeds[\"train\"])\n",
    "val_embeds = np.concatenate(embeds[\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train both a ground truth classifier and a probe for evidence of tamper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training classifiers\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_untrusted_positive_mask(ds: MeasurementDataset) -> torch.Tensor:\n",
    "    return ~ds.is_trusted & ds.measurements.all(-1)\n",
    "\n",
    "train_gt_mask = get_untrusted_positive_mask(train_ds).numpy()\n",
    "train_gt_embeds = train_embeds[train_gt_mask]\n",
    "train_gt_labels = train_ds.ground_truth.numpy()[train_gt_mask]\n",
    "\n",
    "gt_classifier = LogisticRegression(max_iter=1000)\n",
    "gt_classifier.fit(train_gt_embeds, train_gt_labels)\n",
    "\n",
    "# 1 = concistent measurments, 0 = inconsistent measurements\n",
    "z = (train_ds.measurements.all(-1) | ~train_ds.measurements.any(-1))\n",
    "train_evidence_mask = (train_ds.is_trusted | ~z).numpy()\n",
    "train_evidence_embeds = train_embeds[train_evidence_mask]\n",
    "train_evidence_labels = z.numpy()[train_evidence_mask]\n",
    "\n",
    "evidence_classifier = LogisticRegression(max_iter=1000)\n",
    "evidence_classifier.fit(train_evidence_embeds, train_evidence_labels)\n",
    "print(\"Done training classifiers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real vs Fake AUROC using ground truth labels: 0.86\n",
      "Real vs Fake AUROC using evidence of tamper (without ground truth labels): 0.77\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Only evaluate real positive vs fake positive\n",
    "val_positive_mask = get_untrusted_positive_mask(val_ds).numpy()\n",
    "val_positive_embeds = val_embeds[val_positive_mask]\n",
    "val_positive_labels = val_ds.ground_truth.numpy()[val_positive_mask]\n",
    "\n",
    "for classifier_name, classifier in [(\"using ground truth labels\", gt_classifier),\n",
    "                                    (\"using evidence of tamper (without ground truth labels)\", evidence_classifier)]:\n",
    "    classifier: LogisticRegression\n",
    "    pred_probs = classifier.predict_proba(val_positive_embeds)[:, 1]\n",
    "    auroc = roc_auc_score(val_positive_labels, pred_probs)\n",
    "    print(f\"Real vs Fake AUROC {classifier_name}: {auroc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
