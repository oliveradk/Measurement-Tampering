import ast
import itertools
import logging
import subprocess
import threading
from functools import lru_cache
from math import ceil
from pathlib import Path
from typing import Callable, Optional, TypeVar, Union

import attrs
from tqdm import tqdm

from func_correct.exec_code import ExecutionResults, Results, sandboxed_check_correctness
from func_correct.loaded_problem import (
    RRFS_DIR,
    InputOutputTestCases,
    LoadedProblem,
    PythonTestCases,
    TestCases,
    ToEval,
    get_converter,
)

Solution = str

CorrectnessResult = bool


@attrs.frozen
class LoadedProblemAndResults:
    problem: LoadedProblem
    results_list: list[tuple[Solution, Results]]


@attrs.frozen
class EvalConfig:
    timeout_per_answer: float = 0.02
    tqdm_batch_size: Optional[int] = 500
    n_threads: int = 10
    cleanup_containers_after_run: bool = True


DEFAULT_EVAL_CONFIG = EvalConfig()

this_folder = str(Path(__file__).parent)
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
logger.addHandler(logging.FileHandler(f"{this_folder}/evaluate_correctness.log", "w"))


def evaluate_correctness(
    problems_and_answers: list[tuple[LoadedProblem, list[Solution]]],
    eval_config: EvalConfig = DEFAULT_EVAL_CONFIG,
) -> list[LoadedProblemAndResults]:
    """Labels the answers with correctness results.

    If tqmd_batch_size>1, display a progress bar. Don't set it too low as there is a ~2s overhead per batch."""

    codes_and_n_expected_results = []
    for problem, answers in problems_and_answers:
        for answer in answers:
            codes_and_n_expected_results.append(
                (assert_code(answer, problem.test_cases, eval_config.timeout_per_answer), problem.test_cases.n)
            )

    def run(codes_and_n_expected_results):
        codes = [code for code, _ in codes_and_n_expected_results]
        n_expected_results = [n for _, n in codes_and_n_expected_results]
        return sandboxed_check_correctness(codes, n_expected_results, eval_config.timeout_per_answer)

    results_list = run_batched_parallel(
        run, codes_and_n_expected_results, eval_config.tqdm_batch_size, eval_config.n_threads
    )

    i = 0
    res = []
    for problem, answers in problems_and_answers:
        pb_results_list = []
        for answer in answers:
            results = results_list[i]
            i += 1
            pb_results_list.append((answer, results))
        res.append(LoadedProblemAndResults(problem, pb_results_list))

    if eval_config.cleanup_containers_after_run:
        cleanup_containers()
    return res


@lru_cache(maxsize=1)
def get_setup_lines() -> str:
    return (Path(__file__).parent / "exec" / "setup_lines.py").read_text()


def assert_code(code: str, test_cases: TestCases, timeout: float = 1) -> str:
    if isinstance(test_cases, InputOutputTestCases):
        r = assert_code_io_test_cases(code, test_cases)
    elif isinstance(test_cases, PythonTestCases):
        r = assert_code_python_test_cases(code, test_cases)
    else:
        raise ValueError(f"Unknown test case type: {type(test_cases)}")

    time_out_lines = f"time_limit____={timeout}\n" if timeout else ""

    return f"{get_setup_lines()}\n{time_out_lines}{r}"


def assert_code_python_test_cases(code: str, test_cases: PythonTestCases) -> str:
    if test_cases.is_solution_class:
        raise NotImplementedError("assert_code_python_test_cases does not support solution classes")

    lines = ["with time_limiter____():"]
    lines += [f"    {line}" for line in code.splitlines()]
    lines += [f"test_code____({repr(statement)})" for statement in test_cases.boolean_statements()]
    return "\n".join(lines)


def assert_code_io_test_cases(code: str, test_cases: InputOutputTestCases) -> str:
    # Change the meaning of input and print
    lines = indent_into_function(code.splitlines())

    for inp, out in zip(test_cases.inputs, test_cases.outputs):
        lines.append(f"test_io___(f____, {repr(inp)}, {repr(out)})\n")

    return "\n".join(lines)


def complete_test_cases(
    problems_and_tests: list[tuple[LoadedProblem, TestCases]],
    max_solutions_used: int = 3,  # use the first n solutions (throw test if they don't agree)
    eval_config: EvalConfig = DEFAULT_EVAL_CONFIG,
) -> list[tuple[LoadedProblem, TestCases]]:
    """Complete the test cases with the solutions.

    This is useful for the case where the test cases only have inputs, and you have correct solutions."""

    codes_and_n_expected_results = []
    for problem, test_cases in problems_and_tests:
        if isinstance(test_cases, PythonTestCases) and test_cases.is_solution_class:
            raise NotImplementedError("complete_test_cases does not support solution classes")

        solutions_used = problem.solutions[:max_solutions_used]
        n_solution_used = len(solutions_used)

        time_out_line = f"time_limit____={eval_config.timeout_per_answer * n_solution_used}"
        lines = [*get_setup_lines().splitlines(), time_out_line]

        if n_solution_used == 0 or len(test_cases.inputs) == 0:
            code = "\n".join(lines)
            codes_and_n_expected_results.append((code, 0))
        else:
            for i, solution in enumerate(solutions_used):
                if isinstance(test_cases, InputOutputTestCases):
                    lines += indent_into_function(
                        solution.splitlines(),
                        name=f"solution_{i}____",
                    )
                else:
                    # isolate each function
                    args = [f"arg_{j}____" for j in range(len(test_cases.inputs[0]))]
                    lines += indent_into_function(
                        solution.splitlines() + [f"return {test_cases.fn_name}({', '.join(args)})"],
                        name=f"solution_{i}____",
                        args_names=args,
                    )

            if isinstance(test_cases, InputOutputTestCases):
                for inp in test_cases.inputs:
                    solution_list_str = ", ".join([f"solution_{i}____" for i in range(n_solution_used)])
                    lines.append(f"exec_io___([{solution_list_str}], {repr(inp)})")
            else:
                for arg_strs in test_cases.arg_strings():
                    call_strings = [repr(f"solution_{i}____{arg_strs}") for i in range(n_solution_used)]
                    codes_list_str = ", ".join(call_strings)
                    lines.append(f"exec_code___([{codes_list_str}])")

            code = "\n".join(lines)
            codes_and_n_expected_results.append((code, len(test_cases.inputs)))

    def run(codes_and_res):
        codes = [code for code, _ in codes_and_res]
        n_expected_results = [n for _, n in codes_and_res]
        return sandboxed_check_correctness(codes, n_expected_results, eval_config.timeout_per_answer, just_run=True)

    results_list = run_batched_parallel(
        run, codes_and_n_expected_results, eval_config.tqdm_batch_size, eval_config.n_threads
    )

    new_problems_and_tests = []
    for (problem, test_cases), (i, results) in zip(problems_and_tests, enumerate(results_list)):
        if results.failure is not None or (len(problem.solutions) == 0 or max_solutions_used <= 0):
            pb = f"failure\n{results.failure[:200]}" if results.failure is not None else "no solutions"
            logger.error(f"WARNING: Test cases all thrown because of {pb}.")
            logger.debug(f"generated code:\n{codes_and_n_expected_results[i][0]}")
            logger.debug(f"expected {codes_and_n_expected_results[i][1]} results")
            new_test_cases = attrs.evolve(test_cases, inputs=[], outputs=[])
        else:
            assert len(results.results) == len(
                test_cases.inputs
            ), f"Expected {len(test_cases.inputs)} results but got {len(results.results)}"
            new_inputs = []
            new_outputs = []
            for result, test_inps in zip(results.results, test_cases.inputs):
                if result.code is not None:
                    new_inputs.append(test_inps)
                    new_outputs.append(result.code)
            new_test_cases = attrs.evolve(test_cases, inputs=new_inputs, outputs=new_outputs)
        new_problems_and_tests.append((problem, new_test_cases))

    if eval_config.cleanup_containers_after_run:
        cleanup_containers()

    return new_problems_and_tests


T = TypeVar("T")
V = TypeVar("V")


def run_batched_parallel(
    fn: Callable[[list[T]], list[V]],
    data: list[T],
    batch_size: Optional[int] = None,
    n_threads: int = 1,
    min_batch_size: int = 10,
) -> list[V]:
    """Run a function in parallel on a list of data, in batches.

    Use naive split otherwise there are pickling issues.

    batch size less than full batch allows printing. No batch size means no printing."""

    n_threads = min(n_threads, ceil(len(data) / min_batch_size))

    indexed_data = list(enumerate(data))

    data_per_thread = [indexed_data[i::n_threads] for i in range(n_threads)]

    all_results = [None] * len(data)

    number_of_batches = sum(ceil(len(d) / batch_size) for d in data_per_thread) if batch_size is not None else 0
    pbar = tqdm(total=number_of_batches, disable=batch_size is None)
    pbar_lock = threading.Lock()
    results_lock = threading.Lock()

    def run_thread(i, data):
        batches = (
            [data[j : j + batch_size] for j in range(0, len(data), batch_size)] if batch_size is not None else [data]
        )
        res = []

        for j, b in enumerate(batches):
            logger.debug(f"thread {i} batch {j} / {len(batches)}")
            b_data = [d for _, d in b]
            r = fn(b_data)
            res += list(zip([i for i, _ in b], r))

            with pbar_lock:
                pbar.update(1)

        with results_lock:
            for i, r in res:
                all_results[i] = r

    threads = [threading.Thread(target=run_thread, args=(i, data)) for i, data in enumerate(data_per_thread)]

    for t in threads:
        t.start()
    for t in threads:
        t.join()

    pbar.close()

    return all_results  # type: ignore


def indent_into_function(lines: list[str], name: str = "f____", args_names: list[str] = []) -> list[str]:
    """Indent a list of lines into a function definition.

    import math
    print(input())
    ->
    import math
    def f____():
        print(input())"""

    def import_filter(line):
        return line.startswith("import ") or line.startswith("from ")

    import_lines = [line for line in lines if import_filter(line)]
    other_lines = [line for line in lines if not import_filter(line)]
    declaration = f"def {name}({', '.join(args_names)}):"
    return import_lines + [declaration] + ["    " + line for line in other_lines]


def cleanup_containers():
    cmd = "sudo docker ps -a --format '{{.ID}} {{.Command}}' | grep 'python ./unsafe' | awk '{print $1}' | xargs -r sudo docker kill"
    subprocess.run(cmd, shell=True)
